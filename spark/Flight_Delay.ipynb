{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAT500 - FinalProject, Flight Delay Prediction\n",
    "\n",
    "Group Members:\n",
    "\n",
    "    - Brage Solheim\n",
    "    - Yohannes D. Kassaye"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have used map reduce jobs to find null values in our data, find mean and mode of each column, and fill null values with mean or mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initlization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/spark'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORIGINAL_DATA_PATH = \"hdfs:///data/airline_delay_data.csv\"\n",
    "CLEANED_DATA_PATH = \"hdfs:///data/airline_delay_data_cleaned.csv\"\n",
    "\n",
    "TEST_DATA_ORIGINAL_PATH = \"/data/2015.sample.csv\"\n",
    "TEST_DATA_CLEANED_PATH = \"/data/2015.sample.cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "conf = pyspark.SparkConf().setAll([(\"spark.driver.maxResultSize\", '4g'), (\"spark.sql.execution.arrow.pyspark.enabled\", 'true')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2022-04-18 20:52:57,124 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "2022-04-18 20:52:58,724 WARN util.Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "2022-04-18 20:53:03,552 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession\n",
    "            .builder\n",
    "            .master(\"yarn\") # for cluster\n",
    "            # .master(\"local[*]\") # for local testing\n",
    "            .config(conf=conf)\n",
    "            .appName(\"Flight-delay-prediction\")\n",
    "            .getOrCreate())\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('spark.driver.port', '40109'), ('spark.sql.warehouse.dir', 'file:/home/ubuntu/DAT500-FinalProject/spark/spark-warehouse'), ('spark.app.startTime', '1650315176982'), ('spark.executor.id', 'driver'), ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES', 'http://namenode:8088/proxy/application_1645016454320_0102'), ('spark.app.name', 'Flight-delay-prediction'), ('spark.ui.proxyBase', '/proxy/application_1645016454320_0102'), ('spark.app.id', 'application_1645016454320_0102'), ('spark.driver.host', 'namenode'), ('spark.driver.appUIAddress', 'http://namenode:4041'), ('spark.master', 'yarn'), ('spark.executorEnv.PYTHONPATH', '/usr/local/spark/python/:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.9.3-src.zip'), ('spark.rdd.compress', 'True'), ('spark.serializer.objectStreamReset', '100'), ('spark.submit.pyFiles', ''), ('spark.yarn.isPython', 'true'), ('spark.submit.deployMode', 'client'), ('spark.ui.filters', 'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'), ('spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS', 'namenode'), ('spark.ui.showConsoleProgress', 'true'), ('spark.sql.execution.arrow.pyspark.enabled', 'true'), ('spark.driver.maxResultSize', '4g')]\n"
     ]
    }
   ],
   "source": [
    "print(sc.getConf().getAll())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType, DoubleType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"FL_DATE\",\"OP_CARRIER\",\"OP_CARRIER_FL_NUM\",\"ORIGIN\",\"DEST\",\"CRS_DEP_TIME\",\"DEP_TIME\",\"DEP_DELAY\",\"TAXI_OUT\",\"WHEELS_OFF\",\"WHEELS_ON\",\"TAXI_IN\",\"CRS_ARR_TIME\",\"ARR_TIME\",\"ARR_DELAY\",\"CANCELLED\",\"DIVERTED\",\"CRS_ELAPSED_TIME\",\"ACTUAL_ELAPSED_TIME\",\"AIR_TIME\",\"DISTANCE\",\"CARRIER_DELAY\",\"WEATHER_DELAY\",\"NAS_DELAY\",\"SECURITY_DELAY\",\"LATE_AIRCRAFT_DELAY\"]\n",
    "nullable=False\n",
    "schema = StructType([\n",
    "    StructField(\"FL_DATE\", DateType(), nullable), StructField(\"OP_CARRIER\", StringType(), nullable), StructField(\"OP_CARRIER_FL_NUM\", DoubleType(), nullable),\n",
    "    StructField(\"ORIGIN\", StringType(), nullable), StructField(\"DEST\", StringType(), nullable), StructField(\"CRS_DEP_TIME\", DoubleType(), nullable),\n",
    "    StructField(\"DEP_TIME\", DoubleType(), nullable), StructField(\"DEP_DELAY\", DoubleType(), nullable), StructField(\"TAXI_OUT\", DoubleType(), nullable),\n",
    "    StructField(\"WHEELS_OFF\", DoubleType(), nullable), StructField(\"WHEELS_ON\", DoubleType(), nullable), StructField(\"TAXI_IN\", DoubleType(), nullable),\n",
    "    StructField(\"CRS_ARR_TIME\", DoubleType(), nullable), StructField(\"ARR_TIME\", DoubleType(), nullable), StructField(\"ARR_DELAY\", DoubleType(), nullable),\n",
    "    StructField(\"CANCELLED\", DoubleType(), nullable), StructField(\"DIVERTED\", DoubleType(), nullable), StructField(\"CRS_ELAPSED_TIME\", DoubleType(), nullable),\n",
    "    StructField(\"ACTUAL_ELAPSED_TIME\", DoubleType(), nullable), StructField(\"AIR_TIME\", DoubleType(), nullable), StructField(\"DISTANCE\", DoubleType(), nullable),\n",
    "    StructField(\"CARRIER_DELAY\", DoubleType(), nullable), StructField(\"WEATHER_DELAY\", DoubleType(), nullable), StructField(\"NAS_DELAY\", DoubleType(), nullable),\n",
    "    StructField(\"SECURITY_DELAY\", DoubleType(), nullable), StructField(\"LATE_AIRCRAFT_DELAY\", DoubleType(), nullable)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 20:53:38,934 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+\n",
      "|   FL_DATE|OP_CARRIER|OP_CARRIER_FL_NUM|ORIGIN|DEST|CRS_DEP_TIME|DEP_TIME|DEP_DELAY|TAXI_OUT|WHEELS_OFF|WHEELS_ON|TAXI_IN|CRS_ARR_TIME|ARR_TIME|ARR_DELAY|CANCELLED|DIVERTED|CRS_ELAPSED_TIME|ACTUAL_ELAPSED_TIME|AIR_TIME|DISTANCE|CARRIER_DELAY|WEATHER_DELAY|NAS_DELAY|SECURITY_DELAY|LATE_AIRCRAFT_DELAY|\n",
      "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+\n",
      "|2016-09-25|        AA|           2622.0|   EGE| DFW|       800.0|   749.0|    -11.0|    11.0|     800.0|   1049.0|   15.0|      1110.0|  1104.0|     -6.0|      0.0|     0.0|           130.0|              135.0|   109.0|   721.0|        19.62|         3.04|    15.08|          0.09|              24.72|\n",
      "|2016-09-25|        AA|           2623.0|   DFW| TPA|      1830.0|  1915.0|     45.0|    20.0|    1935.0|   2243.0|    3.0|      2155.0|  2246.0|     51.0|      0.0|     0.0|           145.0|              151.0|   128.0|   929.0|          0.0|          0.0|      6.0|           0.0|               45.0|\n",
      "+----------+----------+-----------------+------+----+------------+--------+---------+--------+----------+---------+-------+------------+--------+---------+---------+--------+----------------+-------------------+--------+--------+-------------+-------------+---------+--------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(CLEANED_DATA_PATH, header=True, schema=schema, inferSchema=False)\n",
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.repartition(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 20:53:46,950 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2022-04-18 20:53:47,256 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2022-04-18 20:54:54,136 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FL_DATE</th>\n",
       "      <th>OP_CARRIER</th>\n",
       "      <th>OP_CARRIER_FL_NUM</th>\n",
       "      <th>ORIGIN</th>\n",
       "      <th>DEST</th>\n",
       "      <th>CRS_DEP_TIME</th>\n",
       "      <th>DEP_TIME</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "      <th>WHEELS_OFF</th>\n",
       "      <th>WHEELS_ON</th>\n",
       "      <th>TAXI_IN</th>\n",
       "      <th>CRS_ARR_TIME</th>\n",
       "      <th>ARR_TIME</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>CANCELLED</th>\n",
       "      <th>DIVERTED</th>\n",
       "      <th>CRS_ELAPSED_TIME</th>\n",
       "      <th>ACTUAL_ELAPSED_TIME</th>\n",
       "      <th>AIR_TIME</th>\n",
       "      <th>DISTANCE</th>\n",
       "      <th>CARRIER_DELAY</th>\n",
       "      <th>WEATHER_DELAY</th>\n",
       "      <th>NAS_DELAY</th>\n",
       "      <th>SECURITY_DELAY</th>\n",
       "      <th>LATE_AIRCRAFT_DELAY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>None</td>\n",
       "      <td>OP_CARRIER</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ORIGIN</td>\n",
       "      <td>DEST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  FL_DATE  OP_CARRIER  OP_CARRIER_FL_NUM  ORIGIN  DEST  CRS_DEP_TIME  DEP_TIME  DEP_DELAY  TAXI_OUT  WHEELS_OFF  WHEELS_ON  TAXI_IN  CRS_ARR_TIME  ARR_TIME  ARR_DELAY  CANCELLED  DIVERTED  CRS_ELAPSED_TIME  ACTUAL_ELAPSED_TIME  AIR_TIME  DISTANCE  CARRIER_DELAY  WEATHER_DELAY  NAS_DELAY  SECURITY_DELAY  LATE_AIRCRAFT_DELAY\n",
       "0    None  OP_CARRIER                NaN  ORIGIN  DEST           NaN       NaN        NaN       NaN         NaN        NaN      NaN           NaN       NaN        NaN        NaN       NaN               NaN                  NaN       NaN       NaN            NaN            NaN        NaN             NaN                  NaN"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header_row = df.filter(F.col(\"FL_DATE\").isNull())\n",
    "header_row.to_pandas_on_spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is one row with null values do to the header being in the middle of our data, and when we infer the schema, the header row is not the same type as the rest of the rows, and therefore is converted to null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter((F.col(\"FL_DATE\").isNull() & F.col(\"OP_CARRIER\").isNotNull() & F.col(\"ORIGIN\").isNotNull() & F.col(\"DEST\").isNotNull()) == False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:=======================================================> (35 + 1) / 36]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "header_row = df.filter(F.col(\"FL_DATE\").isNull())\n",
    "print(header_row.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have cleaned the data by removing and filling null values.\n",
    "Now we will try to find the correlation between the our target, DEP_DELAY, and the other columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCorrelationMatrix(df):\n",
    "    inputcols = [col[0] for col in df.dtypes if col[1] == \"double\" or col[1] == \"int\"]\n",
    "    vector_col = \"features\"\n",
    "\n",
    "    assembler = VectorAssembler(inputCols=inputcols, outputCol=vector_col, handleInvalid=\"skip\")\n",
    "    df_vector = assembler.transform(df).select(vector_col)\n",
    "    matrix = Correlation.corr(df_vector, \"features\")\n",
    "\n",
    "    matrix_df = spark.createDataFrame(matrix.collect()[0][\"pearson({})\".format(vector_col)].toArray().tolist(), inputcols).to_pandas_on_spark()\n",
    "    matrix_df[\"feature\"] = inputcols\n",
    "\n",
    "    return matrix_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "corr_matrix = GetCorrelationMatrix(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 21:00:07,951 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2022-04-18 21:00:08,178 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>DEP_DELAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.947662</td>\n",
       "      <td>ARR_DELAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.524859</td>\n",
       "      <td>CARRIER_DELAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.420768</td>\n",
       "      <td>LATE_AIRCRAFT_DELAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.215824</td>\n",
       "      <td>WEATHER_DELAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.147498</td>\n",
       "      <td>DEP_TIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.137768</td>\n",
       "      <td>WHEELS_OFF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.121006</td>\n",
       "      <td>NAS_DELAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.105527</td>\n",
       "      <td>CRS_DEP_TIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.090892</td>\n",
       "      <td>CRS_ARR_TIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.061468</td>\n",
       "      <td>TAXI_OUT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.041366</td>\n",
       "      <td>WHEELS_ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.033067</td>\n",
       "      <td>ARR_TIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.024106</td>\n",
       "      <td>ACTUAL_ELAPSED_TIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.020847</td>\n",
       "      <td>CRS_ELAPSED_TIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.020334</td>\n",
       "      <td>DIVERTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.016649</td>\n",
       "      <td>DISTANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.016027</td>\n",
       "      <td>AIR_TIME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.015874</td>\n",
       "      <td>SECURITY_DELAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.012217</td>\n",
       "      <td>TAXI_IN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.006036</td>\n",
       "      <td>OP_CARRIER_FL_NUM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.005375</td>\n",
       "      <td>CANCELLED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    DEP_DELAY              feature\n",
       "3    1.000000            DEP_DELAY\n",
       "10   0.947662            ARR_DELAY\n",
       "17   0.524859        CARRIER_DELAY\n",
       "21   0.420768  LATE_AIRCRAFT_DELAY\n",
       "18   0.215824        WEATHER_DELAY\n",
       "2    0.147498             DEP_TIME\n",
       "5    0.137768           WHEELS_OFF\n",
       "19   0.121006            NAS_DELAY\n",
       "1    0.105527         CRS_DEP_TIME\n",
       "8    0.090892         CRS_ARR_TIME\n",
       "4    0.061468             TAXI_OUT\n",
       "6    0.041366            WHEELS_ON\n",
       "9    0.033067             ARR_TIME\n",
       "14   0.024106  ACTUAL_ELAPSED_TIME\n",
       "13   0.020847     CRS_ELAPSED_TIME\n",
       "12   0.020334             DIVERTED\n",
       "16   0.016649             DISTANCE\n",
       "15   0.016027             AIR_TIME\n",
       "20   0.015874       SECURITY_DELAY\n",
       "7    0.012217              TAXI_IN\n",
       "0    0.006036    OP_CARRIER_FL_NUM\n",
       "11   0.005375            CANCELLED"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_matrix[[\"DEP_DELAY\", \"feature\"]].sort_values(by=\"DEP_DELAY\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are no negative correlations between the target and the other columns. So we dont need to remove any columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO - need to figure out how to encode date columns (unix_timestamp?? )\n",
    "I think we have a way to encode categorical data using string encoder.\n",
    "\n",
    "These two is needed for the vector assembler!\n",
    "\n",
    "Oncce that is done we start on selecting a regression model, and start predicting.\n",
    "If we are done early with that we can try to optimize the time spark / hadoop - mapreduce tasks take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-18 21:05:02,975 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "2022-04-18 21:05:02,988 WARN window.WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "[Stage 37:=================>                                      (11 + 2) / 36]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, unix_timestamp\n",
    "df.withColumn(\"FL_DATE_ENC\", unix_timestamp(col(\"FL_DATE\"), format=\"yyyy-MM-dd\").cast(\"timestamp\")).to_pandas_on_spark().head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
